{"cells":[{"cell_type":"markdown","metadata":{"id":"-VBIY5ueX5cm"},"source":["<br /><h1> <font color=\"blue\"> <b> Part III: Controls </b></h1> <br />\n","\n","# Politeness control\n","\n","Some aspects of generation can be controlled thanks to special tokens in the input. For instance multi-domain models can be trained and used using source-side domain tags (https://aclanthology.org/R17-1049).\n","\n","This work https://aclanthology.org/N16-1005/ used special tokens to control the politeness of the output.\n","\n","We will implement this approach for English-French translation, to control the use of \"tu\" VS \"vous\" pronouns, which are formal/informal translations of \"you\".\n","\n","We only need to partition the training data into formal VS informal splits, by looking for occurrences of \"tu\" and \"vous\". Then, add source-side control tags depending on the politeness level of the target, and train the model with this.\n","At test time, we only need to put the right control tag and the model will know how to interpret it to pick the right level of politeness.\n","\n","\n","## Preparing the data\n","\n","As we only rely on the \"politeness control token,\" it is necessary to prepare distinctive polite and non-polite training samples from the corpus.\n","\n","While a lot of different aspects of French grammar can be considered here, to start with, we pick sentences that contain \"tu\" and \"vous\" — both meaning \"you\"  in English — and label them as \"non-polite\" and \"polite,\" respectively.\n","\n","### Regular expressions\n","\n","To extract the sentences that contain the words \"tu\" or \"vous\", we can use the following regular expressions:\n","```python\n","r'(^|\\W)(vous)(\\W|$)'\n","r'(^|\\W)(tu)(\\W|$)'\n","```\n","They match sentences that contain the corresponding words by making sure that each word is preceded and followed by a \"non-word\" character (e.g., whitespace or dash)\n","\n","For more information on regexes, you can check out the following resources:\n","- https://www.regular-expressions.info/tutorial.html\n","- https://docs.python.org/3/library/re.html\n","- https://regex101.com/#python"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qbg9TWkkX5cn"},"outputs":[],"source":["def is_formal(line):\n","    \"\"\"\n","    Contains formal French translations of \"you\"\n","    \"\"\"\n","    regex = r'(^|\\W)(vous)(\\W|$)'\n","    return bool(re.search(regex, line, re.IGNORECASE))\n","\n","def is_informal(line):\n","    \"\"\"\n","    Contains informal French translations of \"you\"\n","    \"\"\"\n","    regex = r'(^|\\W)(tu)(\\W|$)'\n","    return bool(re.search(regex, line, re.IGNORECASE))"]},{"cell_type":"markdown","metadata":{"id":"rWFyiImyX5cn"},"source":["### Adding politeness control tags\n","\n","When we identify sentences that are either polite or non-polite, we can attach corresponding control tags in front of each sentence."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N1uexVKUX5cn"},"outputs":[],"source":["def preprocess_formal(source_line, target_line=None, source_lang=None, target_lang=None):\n","    \"\"\"\n","    Tokenizes the given line pair and prepends the <formal> source-side tag\n","    \"\"\"\n","    source_line, target_line = preprocess(source_line, target_line)\n","    source_line = f'<formal> {source_line}'\n","    return source_line, target_line\n","\n","def preprocess_informal(source_line, target_line=None, source_lang=None, target_lang=None):\n","    \"\"\"\n","    Tokenizes the given line pair and prepends the <informal> source-side tag\n","    \"\"\"\n","    source_line, target_line = preprocess(source_line, target_line)\n","    source_line = f'<informal> {source_line}'\n","    return source_line, target_line\n","\n","def preprocess_formal_or_informal(source_line, target_line, source_lang=None, target_lang=None):\n","    \"\"\"\n","    Preprocessing function for politeness control:\n","    - keep only line pairs whose target side has French formal or informal pronouns\n","    - prepend politeness control tags to the source side\n","    \"\"\"\n","    if is_formal(target_line):\n","        return preprocess_formal(source_line, target_line)\n","    elif is_informal(target_line):\n","        return preprocess_informal(source_line, target_line)\n","    else:  # this line pair in neither formal nor informal\n","        # This example will be filtered out by load_dataset (uncomment below to keep it, without a control tag):\n","        # return preprocess(source_line, target_line)\n","        return None"]},{"cell_type":"markdown","metadata":{"id":"knc7-9ZKX5cn"},"source":["### Filtering and loading the dataset\n","\n","Finally, we can filter and load the dataset by passing the `preprocess_formal_or_informal` function to `load_dataset`.\n","This will keep only the line pairs that contain formal or informal pronouns and preprocess the sources to add control tags."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"afj-U7-VX5cn"},"outputs":[],"source":["# Use the same dataset as before\n","train_path = os.path.join(data_dir, 'train.en-fr')\n","valid_path = os.path.join(data_dir, 'valid.en-fr')\n","\n","# But preprocess it to keep only line pairs that use tu/vous pronouns and to append control tags\n","train_data = load_dataset(\n","    train_path, 'en', 'fr',\n","    preprocess=preprocess_formal_or_informal,\n",")\n","\n","valid_data = load_dataset(\n","    valid_path, 'en', 'fr',\n","    preprocess=preprocess_formal_or_informal,\n",")"]},{"cell_type":"markdown","metadata":{"id":"VMDMRNjBX5cn"},"source":["## Setting up for training\n","\n","As we are introducing new vocabularies (i.e., the control tokens), we need to add them to our pretrained model's existing vocabulary.\n","\n","Here, we replace the last two most infrequent tokens so that we do not need to resize the vocabulary and embeddings.\n","\n","Note that the replaced words will now be mapped to UNK."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-uw6IeU2X5cn"},"outputs":[],"source":["source_dict = rnn_attn_model.source_dict\n","print(source_dict)\n","\n","# Replace some infrequent tokens with the new control tokens (these words will now be mapped to UNK)\n","# This is a bit dirty, but this way we don't have to resize the pretrained model's vocabulary and embeddings\n","source_dict[len(source_dict) - 2] = '<formal>'\n","source_dict[len(source_dict) - 1] = '<informal>'\n","\n","# Binarize the training and validation data with these vocabularies\n","binarize(train_data, source_dict, target_dict, sort=True)\n","binarize(valid_data, source_dict, target_dict, sort=False)\n","\n","# You can see that the training source examples now start with special tokens.\n","print(train_data[:5])\n","\n","print('train_size={}, valid_size={}, min_len={}, max_len={}, avg_len={:.1f}'.format(\n","    len(train_data),\n","    len(valid_data),\n","    train_data['source_len'].min(),\n","    train_data['source_len'].max(),\n","    train_data['source_len'].mean(),\n","))\n","\n","reset_seed()\n","\n","train_iterator = BatchIterator(train_data, 'en', 'fr', batch_size=512, max_len=30, shuffle=True)\n","valid_iterator = BatchIterator(valid_data, 'en', 'fr', batch_size=512, max_len=30, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ubDVlTJqX5co"},"outputs":[],"source":["# Finetune the EN-FR pretrained model with the new data\n","new_checkpoint_path = os.path.join(model_root, 'en-fr', 'rnn-attn.pt')\n","rnn_attn_model.reset_optimizer()\n","\n","train_model(rnn_attn_model, train_iterator, [valid_iterator], new_checkpoint_path, epochs=5)"]},{"cell_type":"markdown","metadata":{"id":"8aaJZ5tsX5co"},"source":["## Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hB9jICBbX5co"},"outputs":[],"source":["translate(rnn_attn_model, \"would you lend me your bicycle ?\", preprocess_formal, 'en', 'fr')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D_2PAYvXX5co"},"outputs":[],"source":["translate(rnn_attn_model, \"would you lend me your bicycle ?\", preprocess_informal, 'en', 'fr')"]},{"cell_type":"markdown","metadata":{"id":"4HqECXbK2_O-"},"source":["<br /><br /><br /><hr width=170% /><br />\n","<h2><font color=green><b>EXERCISE 3:</b> Politeness control in Dutch</font> </h2>\n","\n","\n","<font color=green><b></b></font>&nbsp;&nbsp;&nbsp;\n","\n","Implement politeness control for Dutch. Below, we already added code to download the data for English-Dutch and to train a new BPE. In order to complete this exercise, you should train a new model for English-Dutch, define new functions for <em>is_formal()</em> and <em>is_informal()</em>, use them in preprocessing and finetune the English-Dutch model. Try to come up with the most complete rules and demonstrate that this works by 'formalizing' and 'deformalizing' English to Dutch translations using the translate() function as done above.\n","\n","<br />\n","\n","If you don't speak Dutch, please indicate that in your exercise. We still expect you to complete the exercise but we'll know your knowledge of the grammar rules is limited. For a short intro to politeness control in Dutch, see here: https://blogs.transparent.com/dutch/formal-and-informal-pronouns/. Of course, you can also ask questions about this during the labs.\n","\n","<br /><br /><hr width=170% /><br />\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1d_p4_RfDMhu"},"outputs":[],"source":["# Download preprocessed data for English -> Dutch\n","if not os.path.exists('dev.en-nl.nl'):\n","  !wget https://raw.githubusercontent.com/esther2000/MT-2022/main/valid.en-nl.en  # English validation set\n","  !wget https://raw.githubusercontent.com/esther2000/MT-2022/main/valid.en-nl.nl  # Dutch validation set\n","  !wget https://raw.githubusercontent.com/esther2000/MT-2022/main/test.en-nl.en  # English test set\n","  !wget https://raw.githubusercontent.com/esther2000/MT-2022/main/test.en-nl.nl  # Dutch test set\n","  !wget https://raw.githubusercontent.com/esther2000/MT-2022/main/train.en-nl.en  # English train set\n","  !wget https://raw.githubusercontent.com/esther2000/MT-2022/main/train.en-nl.nl  # Dutch train set\n","\n","# The files are now available\n","! ls train*\n","\n","# The format is the same as that of the previously used data\n","! head -5 train.en-nl.nl"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Btiw1If6OmDu"},"outputs":[],"source":["# Train new BPE for English and Dutch\n","if not os.path.exists('data/bpecodes.en-nl'):\n","  !cat train.en-nl.en train.en-nl.nl | subword-nmt learn-bpe -o data/bpecodes.en-nl -s 8000 -v"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AL_Xl8KPOYtK"},"outputs":[],"source":["# The BPE preprocessing functions should be reloaded, as they use\n","# 'bpe_model', which was changed to accomodate Dutch.\n","\n","with open(bpe_path) as bpe_codes:\n","    bpe_model = BPE(bpe_codes)\n","\n","def preprocess(source_line, target_line=None, source_lang=None, target_lang=None):\n","    source_line = bpe_model.segment(source_line.lower())\n","    if target_line is not None:\n","        target_line = bpe_model.segment(target_line.lower())\n","    return source_line, target_line"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.12"}},"nbformat":4,"nbformat_minor":0}
